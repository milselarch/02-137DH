{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c3f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129dba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c646c9d55b304d34aded7fea8449d710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff404b0f56d4fc0af6df28ad4467882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23002a79e09493280957b60dc626498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://i.redd.it/x1p9q9jngae51.jpg</td>\n",
       "      <td>i1j8x7</td>\n",
       "      <td>user reports:\\n    1: It's promoting hate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://i.redd.it/x1p9q9jngae51.jpg</td>\n",
       "      <td>i1j8x7</td>\n",
       "      <td>This deserves bipartisan support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://i.redd.it/x1p9q9jngae51.jpg</td>\n",
       "      <td>i1j8x7</td>\n",
       "      <td>Proud to have joined this sub when I see posts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://i.redd.it/x1p9q9jngae51.jpg</td>\n",
       "      <td>i1j8x7</td>\n",
       "      <td>A reminder that we are all Americans, and when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://i.redd.it/x1p9q9jngae51.jpg</td>\n",
       "      <td>i1j8x7</td>\n",
       "      <td>Thanks! From a democrat. I don’t see this cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              post_url post_id  \\\n",
       "0  https://i.redd.it/x1p9q9jngae51.jpg  i1j8x7   \n",
       "1  https://i.redd.it/x1p9q9jngae51.jpg  i1j8x7   \n",
       "2  https://i.redd.it/x1p9q9jngae51.jpg  i1j8x7   \n",
       "3  https://i.redd.it/x1p9q9jngae51.jpg  i1j8x7   \n",
       "4  https://i.redd.it/x1p9q9jngae51.jpg  i1j8x7   \n",
       "\n",
       "                                             comment  \n",
       "0      user reports:\\n    1: It's promoting hate ...  \n",
       "1                   This deserves bipartisan support  \n",
       "2  Proud to have joined this sub when I see posts...  \n",
       "3  A reminder that we are all Americans, and when...  \n",
       "4  Thanks! From a democrat. I don’t see this cont...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df = pd.read_csv('../datasets/republican_comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93982437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i1j8x7' 'jotfkb' 'jnihd5' 'i39vxx' 'kixcbm' 'j3qg7g' 'kbqvcb' 'hf616r'\n",
      " 'j7o48v' 'ln4hff' 'hfog0h' 'atkbwd' 'im0v4q' 'khihph' 'ihxj9d' 'k4lsju'\n",
      " 'hyau77' 'irhktt' 'im93ql' 'gyazus' 'j2clx9' 'hxs205' 'iigxvu' 'hjhq2k'\n",
      " 'jn4yd8' 'l2mgkk' 'i23528' 'jbt8zz' 'gwol7q' 'kd1h8h' 'ilfugm' 'i5evdt'\n",
      " 'k11use' 'ihqwet' 'jal8br' 'ixs03c' 'ijlosv' 'isz7e8' 'hxzsri' 'jzkdkx'\n",
      " 'jbbhwx' 'i0ku01' 'joe2bq' 'kav9xs' 'l8xn47' 'knyd6s' 'k98iyb' 'kcm159'\n",
      " 'gzqhfo' 'idhlhr' 'luqpr5' 'i646tq' 'j4h3rq' 'i0b5vv' 'i0e7gb' 'ao4pso'\n",
      " 'm7khnd' 'hl7si6' 'l3t5tg' 'lrlv5w' 'jjdtcr' 'ie3psx' 'jcxdyg' 'nvsdbp'\n",
      " 'lkth2z' 'jzvy3s' 'c63kex' 'j6d2hx' 'kqhsum' 'j7jtyn' 'hcxnat' 'lv92hd'\n",
      " 'i2ogze' 'asul6v' 'odqfem' 'ikoj1u' 'jkknfi' 'is9hvp' 'n93yy5' 'm3j2wq'\n",
      " 'j0alok' 'jgq9w3' 'nrkvy2' 'jfbway' 'i9nr4p' 'lzxy5l' 'i80bw6' 'i9883m'\n",
      " 'ik2enz' 'k9v20m' 'i4dtp0' 'lo7kot' 'i04v99' 'mutklo' 'hn0xpv' 'm49fng'\n",
      " 'igex7g' 'ifaxt6' 'iqtxd2' 'kfr98z'] 100\n"
     ]
    }
   ],
   "source": [
    "post_ids = df['post_id'].unique()\n",
    "print(post_ids, len(post_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53fa8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           user reports:\\n    1: It's promoting hate ...\n",
      "1                        This deserves bipartisan support\n",
      "2       Proud to have joined this sub when I see posts...\n",
      "3       A reminder that we are all Americans, and when...\n",
      "4       Thanks! From a democrat. I don’t see this cont...\n",
      "                              ...                        \n",
      "3453    wow, how peoples politcal views have changed o...\n",
      "3454    How sadly true, today's Republicans are yester...\n",
      "3455    This is do dumb and not true. Things, people a...\n",
      "3456             Bruh actors and aren’t political figures\n",
      "3457    If Charlie Kirk showed up in 1960 conservative...\n",
      "Name: comment, Length: 3458, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d12cefa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406218\n",
      "word [free] count 73\n",
      "word [racist] count 102\n",
      "word [government] count 47\n"
     ]
    }
   ],
   "source": [
    "comments = df['comment'].to_numpy()\n",
    "all_comments = ' '.join(comments)\n",
    "print(len(all_comments))\n",
    "print('word [free] count', all_comments.count('free'))\n",
    "print('word [racist] count', all_comments.count('racist'))\n",
    "print('word [government] count', all_comments.count('government'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff809c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', 'this', 'is', 'a', 'test', 'sentence', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "text = \"Here is the sentence I want embeddings for. This ... is a test sentence\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c067f648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d488dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some of the vocabulary that BERT is trained on\n",
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04ec713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf4025f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad015c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
      "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
      "         ...,\n",
      "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
      "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
      "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]]), tensor([[[ 0.0522,  0.0595, -0.2179,  ...,  0.2280, -0.0712,  0.0148],\n",
      "         [ 0.3819,  0.1475,  0.2414,  ...,  0.3397,  0.7607,  0.4999],\n",
      "         [ 0.1705, -0.6168, -0.7296,  ...,  0.8631,  0.6274, -0.3727],\n",
      "         ...,\n",
      "         [ 0.6982, -0.4554, -1.7845,  ...,  0.3308,  0.0710, -0.5187],\n",
      "         [-0.0905,  0.1862, -0.4437,  ...,  0.2244,  0.1810,  0.3740],\n",
      "         [-0.0825,  0.0466, -0.1526,  ..., -0.2033,  0.3370, -0.1767]]]), tensor([[[-0.0357, -0.2022, -0.4103,  ...,  0.2511,  0.0586, -0.0547],\n",
      "         [ 0.1430,  0.0747,  0.0595,  ..., -0.2914,  0.1733,  0.4265],\n",
      "         [ 0.5752, -1.2385, -0.5650,  ...,  0.8995,  0.4652, -0.8080],\n",
      "         ...,\n",
      "         [ 0.9157, -0.4118, -1.6042,  ...,  0.1454,  0.1699, -0.2230],\n",
      "         [-0.1816,  0.0099,  0.0370,  ..., -0.2178,  0.0481,  0.3477],\n",
      "         [-0.1524, -0.1068, -0.0868,  ..., -0.1369,  0.2633, -0.3754]]]), tensor([[[-0.0183, -0.3853, -0.1600,  ...,  0.2446,  0.2160,  0.0633],\n",
      "         [ 0.7727, -0.3786,  0.6677,  ..., -0.2014,  0.0094,  0.6461],\n",
      "         [ 0.8822, -0.7807, -0.6305,  ...,  0.5223,  0.3687, -0.9875],\n",
      "         ...,\n",
      "         [ 0.9788, -0.0669, -1.6344,  ...,  0.0341,  0.0437, -0.1636],\n",
      "         [-0.4724, -0.0258,  0.3107,  ..., -0.2553, -0.1049,  0.0897],\n",
      "         [-0.0816, -0.1028,  0.0947,  ...,  0.0091,  0.0649, -0.0646]]]), tensor([[[ 0.0207, -0.3990, -0.8255,  ...,  0.3448,  0.0951,  0.3650],\n",
      "         [ 0.5667, -0.7513,  0.0695,  ..., -0.1052, -0.1700,  0.4333],\n",
      "         [ 0.7095, -0.2044, -0.1709,  ...,  0.7092,  0.0542, -1.0380],\n",
      "         ...,\n",
      "         [ 1.3519,  0.3352, -1.3555,  ...,  0.1676, -0.2817,  0.1042],\n",
      "         [-0.2488,  0.1044, -0.1778,  ..., -0.2713, -0.2643,  0.2310],\n",
      "         [-0.0266, -0.0469,  0.0053,  ...,  0.0085,  0.0417, -0.0486]]]), tensor([[[-0.3443, -0.6315, -0.6647,  ...,  0.0050,  0.1167,  0.3614],\n",
      "         [ 0.8173, -0.9999, -0.1752,  ..., -0.4932, -0.2556,  0.2038],\n",
      "         [ 0.8973, -0.2652, -0.6559,  ...,  0.5725,  0.4178, -0.6356],\n",
      "         ...,\n",
      "         [ 1.5582,  0.4479, -1.2054,  ...,  0.2052, -0.5083,  0.3994],\n",
      "         [-0.1898,  0.0898, -0.2766,  ..., -0.3044, -0.4370,  0.4248],\n",
      "         [-0.0264, -0.0364,  0.0304,  ...,  0.0160,  0.0067, -0.0486]]]), tensor([[[-2.3737e-01, -9.9507e-01, -4.5364e-01,  ..., -2.1456e-01,\n",
      "           3.5440e-01,  2.5447e-01],\n",
      "         [ 5.7495e-01, -9.1262e-01, -2.3175e-01,  ..., -1.8444e-01,\n",
      "          -1.8389e-01,  1.3615e-01],\n",
      "         [ 2.5019e-01, -5.6334e-01, -7.0052e-01,  ...,  2.0830e-01,\n",
      "           6.6902e-01,  5.5973e-02],\n",
      "         ...,\n",
      "         [ 1.5415e+00,  3.5867e-03, -1.0563e+00,  ...,  6.0117e-02,\n",
      "          -3.6085e-01,  5.3853e-01],\n",
      "         [-5.8238e-01,  8.6806e-02, -4.4414e-01,  ..., -8.1630e-01,\n",
      "          -4.2633e-01,  4.1039e-01],\n",
      "         [ 1.3974e-02, -5.6850e-02, -4.0641e-03,  ...,  7.8721e-05,\n",
      "          -1.4510e-02, -4.7457e-02]]]), tensor([[[-1.5213e-01, -9.7455e-01, -5.6574e-01,  ..., -6.0024e-01,\n",
      "           3.0606e-01,  3.6930e-01],\n",
      "         [ 2.7012e-01, -5.4540e-01,  1.6653e-02,  ...,  6.2675e-02,\n",
      "           2.6824e-01,  3.4129e-02],\n",
      "         [-4.1255e-01, -6.4567e-01, -2.0088e-01,  ...,  2.3458e-01,\n",
      "           1.4713e-01,  4.0930e-03],\n",
      "         ...,\n",
      "         [ 1.3217e+00, -6.6627e-02, -8.1355e-01,  ..., -3.9645e-01,\n",
      "          -5.2719e-01,  2.0277e-01],\n",
      "         [-5.6073e-01, -4.5253e-01, -5.8201e-01,  ..., -1.4335e+00,\n",
      "           2.8755e-01,  5.9470e-01],\n",
      "         [ 6.2807e-05, -2.2453e-02, -2.1868e-02,  ..., -1.4897e-02,\n",
      "           2.8921e-03, -4.3436e-02]]]), tensor([[[ 0.0967, -0.8567, -0.5056,  ..., -0.4204,  0.1267,  0.3225],\n",
      "         [-0.0109, -0.2976,  0.2986,  ..., -0.3344,  0.2755, -0.1571],\n",
      "         [-0.6171, -0.4909, -0.1630,  ..., -0.3340,  0.4452, -0.2641],\n",
      "         ...,\n",
      "         [ 0.9738,  0.0119, -0.6562,  ..., -0.3114, -0.1033,  0.1980],\n",
      "         [-0.8265, -0.4334, -0.7586,  ..., -1.2721,  0.0482,  0.3751],\n",
      "         [ 0.0196,  0.0017,  0.0323,  ..., -0.0348, -0.0410, -0.0743]]]), tensor([[[-0.3495, -0.7788, -0.7701,  ..., -0.0715,  0.2350,  0.1686],\n",
      "         [-0.2694, -0.3884, -0.3181,  ..., -0.7334,  0.0196, -0.4148],\n",
      "         [-0.5444, -0.3546, -0.2131,  ..., -0.5306,  0.2799, -0.3587],\n",
      "         ...,\n",
      "         [ 0.6356,  0.0980, -0.2775,  ..., -0.6243, -0.2620,  0.1343],\n",
      "         [-0.4167,  0.1132, -0.4761,  ..., -0.5415, -0.1268,  0.0483],\n",
      "         [-0.0671, -0.0451,  0.0232,  ..., -0.0718, -0.0020, -0.0556]]]), tensor([[[-0.9560, -1.0372, -0.8875,  ...,  0.0905, -0.3867,  0.3258],\n",
      "         [-0.3215, -1.1213, -0.2349,  ..., -0.5940,  0.1851, -0.5076],\n",
      "         [-0.4795, -1.0745, -0.1744,  ..., -0.5026,  0.1293, -0.1963],\n",
      "         ...,\n",
      "         [ 0.2736, -0.4021, -0.1525,  ..., -0.7099, -0.7498, -0.0066],\n",
      "         [-0.0143,  0.0187, -0.0490,  ..., -0.0197, -0.0281,  0.0165],\n",
      "         [-0.5765, -0.5547, -0.2122,  ...,  0.2610, -0.4111, -0.1223]]]), tensor([[[-0.6121, -0.6371, -0.8917,  ...,  0.1026, -0.2241,  0.3330],\n",
      "         [-0.2817, -0.6142, -0.4499,  ..., -0.7273,  0.0304, -0.5106],\n",
      "         [-0.4519, -0.2628, -0.1917,  ..., -0.5405, -0.2146, -0.2140],\n",
      "         ...,\n",
      "         [ 0.3213, -0.2997, -0.0471,  ..., -0.8094, -0.7861, -0.0618],\n",
      "         [ 0.0430,  0.0219, -0.0214,  ...,  0.0196, -0.0353,  0.0072],\n",
      "         [-0.0599, -0.6397, -0.6008,  ...,  0.4218, -0.5170, -0.1616]]]), tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
      "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
      "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
      "         ...,\n",
      "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
      "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
      "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]]))\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "    print(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaee63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
